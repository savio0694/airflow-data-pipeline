---
title: "AIRFLOW-DATA PIPELINE"
author: "LEANDER LEITAO"

output:
  
    html_document:
   
runtime: shiny
---

<div>
<img src=https://github.com/savio0694/airflow-pipeline/blob/main/images/airflow.png?raw=true>
</div>


<p><h4>Apache Airflow is an open-source data workflow management platform. It started at Airbnb in October  2015 as a solution to manage the company's increasingly complex workflows. Creating Airflow allowed Airbnb to programmatically author and schedule their  data workflows and monitor them via the built-in Airflow user interface.</h4></p>

<p><h4>Airflow uses directed acyclic graphs (DAGs) to manage workflow orchestration. Tasks and dependencies are defined in Python and then Airflow manages the scheduling and execution. DAGs can be run either on a defined schedule (e.g. hourly or daily) or based on external event triggers .</h4></p>

<br>
<br>

<h2><b> STAGE-1 :DATA GATHERING</b></h2>
<p><h4>The data has been gathered from FINNHUB.IO,an online financial instrument API
that provides data on various american listed companies.</h4></p>

>I have gathered data relating to the public sentiment based on news media for leading companies in the tech sector including GOOGLE,MICROSOFT,ALIBABA,FACEBOOK etc.The data is recieved in JSON format.

```
{'buzz': {'articlesInLastWeek': 61, 'buzz': 0.9682, 'weeklyAverage': 63},
  'companyNewsScore': 0.8,
  'sectorAverageBullishPercent': 0.605,
  'sectorAverageNewsScore': 0.5115,
  'sentiment': {'bearishPercent': 0.1429, 'bullishPercent': 0.8571},
  'symbol': 'NFLX'} 
  ```
  
<br>
<br>

<h2><b> STAGE-2 :DATA TRANSFORMATION</b></h2>
<p><h4> The python data analysis library pandas  transforms the JSON files into a table structured dataframe,arrange companies by bullish(upward trending)  sentiment and save it in csv format.</h4></p>

>The final dataframe looks like so:

<div>
<img src=https://github.com/savio0694/airflow-pipeline/blob/main/images/frame.PNG?raw=true>
</div>

<br>

>Save a copy on my dektop along with the date using UNIX commands

<br>
<img src=https://github.com/savio0694/airflow-pipeline/blob/main/images/dag5.PNG?raw=true>
<br>
<h2><b> STAGE-3 :PIPELINE MONITORING</b></h2>
<p><h4>Send an email once stage 2 has been completed.We can also monitor the individual stages of the pipeline through airflows inbuilt user interface.</h4></p>

<br>
<img src=https://github.com/savio0694/airflow-pipeline/blob/main/images/email.PNG?raw=true>

<br>

<img src=https://github.com/savio0694/airflow-pipeline/blob/main/images/dag1.PNG?raw=true>

<br>
>The pipeline is scheduled to ru at 11:00 am GMT daily

<img src=https://github.com/savio0694/airflow-pipeline/blob/main/images/dag2.PNG?raw=true>

<br>


<h2><b>PYTHON CODE FOR THE DATA TRANSFORMATION/CLEANING</b></h2>
```python

import os
import json
import pandas as pd
from pandas.io.json import json_normalize
import requests
data_json=[]

symbols=['NFLX','MSFT','GOOGL','FB','AMZN','BABA','TSLA']
    
for val in symbols:
    r = requests.get('https://finnhub.io/api/v1/news-sentiment?symbol='+str(val)+'&token=bvv9c7f48v6r5v93tap0')
    data_json.append(r.json())
    df = json_normalize(data_json)

df_cols=df.iloc[:,[1,3,5,6,2,7]]
df_cols.columns=['NewsScore','Symbol','Buzz','WeekAvgBuzz','BearSentiment','BullSentiment']
df_cols=df_cols[['Symbol','NewsScore','Buzz','WeekAvgBuzz','BearSentiment','BullSentiment']]
df_final=df_cols.sort_values(by=['BullSentiment'],ascending=False).reset_index()

df_final.to_csv("/mnt/c/Users/leand/AirflowHome/dags/stocks.csv")
```

<br>
<h2><b>PYTHON CODE FOR THE PIPELINE</b></h2>

```python
from airflow import DAG
from airflow.operators.bash_operator import BashOperator
from airflow.operators.python_operator import PythonOperator
from datetime import datetime, timedelta

from airflow.operators.email import EmailOperator
import papermill as pm


default_args = {
    'owner': 'leander',
    'depends_on_past': False,
    'start_date': datetime.today() - timedelta(days=5),
    'email': ['leandersavio@gmail.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(seconds=10),
}

dag = DAG('MYDAG', default_args=default_args,schedule_interval='0 11 * * *')


 

def DataFetch_Transform_Clean():
    pm.execute_notebook(
    '/mnt/c/users/leand/AirflowHome/dags/stock.ipynb',
    '/mnt/c/users/leand/AirflowHome/dags/stock_out.ipynb',
    parameters=dict(alpha=0.6, ratio=0.1)
     
)

DataFetch_Transform_Clean = PythonOperator(
        task_id="run_notebook",
        python_callable=DataFetch_Transform_Clean,
        dag=dag
    )

move_file = BashOperator(
        task_id='move-file',
        bash_command='echo  $(date) >> /mnt/c/users/leand/desktop/top_picks.csv;cat /mnt/c/users/leand/AirflowHome/dags/stocks.csv>> /mnt/c/users/leand/desktop/top_picks.csv',
        dag=dag

    )


send_email = EmailOperator(
        task_id='send_email',
        to='leandersavio@gmail.com',
        subject='Airflow Alert',
        html_content=""" <h3>HELLO,</h3><br><h3> PIPELINE SUCESSFULL</h3> """,
        dag=dag
)

  
    
DataFetch_Transform_Clean >> move_file >> send_email

```


