---
title: "AIRFLOW-DATA PIPELINE"
author: "LEANDER LEITAO"

output:
  
    html_document:
   
runtime: shiny
---

<div>
<img src=https://github.com/savio0694/airflow-pipeline/blob/main/images/airflow.png?raw=true>
</div>


<p><h4>Apache Airflow is an open-source data workflow management platform. It started at Airbnb in October  2015 as a solution to manage the company's increasingly complex workflows. Creating Airflow allowed Airbnb to programmatically author and schedule their  data workflows and monitor them via the built-in Airflow user interface.</h4></p>

<p><h4>Airflow uses directed acyclic graphs (DAGs) to manage workflow orchestration. Tasks and dependencies are defined in Python and then Airflow manages the scheduling and execution. DAGs can be run either on a defined schedule (e.g. hourly or daily) or based on external event triggers .</h4></p>

<br>
<br>

<h2><b> STAGE-1 :DATA GATHERING</b></h2>
<p><h4>The data has been gathered from FINNHUB.IO,an online financial instrument API
that provides data on various american listed companies.</h4></p>

>I have gathered data relating to the public sentiment based on news media for leading companies in the tech sector including GOOGLE,MICROSOFT,ALIBABA,FACEBOOK etc.

<br>
<br>

<h2><b> STAGE-2 :DATA TRANSFORMATION</b></h2>
<p><h4>We use the python data analysis library pandas to transform the JSON files into a table structured dataframe,arrange companies by bullish(upward trending)  sentiment and save it in csv format.</h4></p>

>The final dataframe looks like so:

<div>
<img src=https://github.com/savio0694/airflow-pipeline/blob/main/images/frame.PNG?raw=true>
</div>

<br>
<br>
<h2><b> STAGE-3 :PIPELINE MONITORING</b></h2>
<p><h4>Send an email once stage 2 has been completed.We can also monitor the individual stages of the pipeline through airflows inbuilt user interface.</h4></p>

<br>
<img src=https://github.com/savio0694/airflow-pipeline/blob/main/images/email.PNG?raw=true>

<br>

<img src=https://github.com/savio0694/airflow-pipeline/blob/main/images/dag1.PNG?raw=true>

<br>
>The pipeline is scheduled to ru at 11:00 am GMT daily
<img src=https://github.com/savio0694/airflow-pipeline/blob/main/images/dag2.PNG?raw=true>

<br>


<h2><b>PYTHON CODE FOR THE PIPELINE</b></h2>

```python
from airflow import DAG
from airflow.operators import BashOperator
from airflow.operators.python_operator import PythonOperator
from datetime import datetime, timedelta
import requests
import pandas as pd
import json
from pandas.io.json import json_normalize 
from airflow.operators.email_operator import EmailOperator

data_json=[]

default_args = {
    'owner': 'leander',
    'depends_on_past': False,
    'start_date': datetime.today() - timedelta(days=13),
    'email': ['leandersavio@gmail.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(seconds=10),
}

dag = DAG('MYDAG', default_args=default_args,schedule_interval='0 11 * * *')

def fetchDataAPI():
    symbols=['NFLX','MSFT','GOOGL','FB','AMZN','BABA','TSLA']
    
    for val in symbols:
        r = requests.get('https://finnhub.io/api/v1/news-sentiment?symbol='+str(val)+'&token=bvv9c7f48v6r5v93tap0')
        data_json.append(r.json())
        df = json_normalize(data_json)
        df.to_csv("/usr/local/airflow/stocks.csv")
        

    
def datatransform():
    df=pd.read_csv("/usr/local/airflow/stocks.csv")
    df_cols=df.iloc[:,[1,3,5,6,2,7]]
    df_cols.columns=['NewsScore','Symbol','Buzz','WeekAvgBuzz','BearSentiment','BullSentiment']
    df_cols=df_cols[['Symbol','NewsScore','Buzz','WeekAvgBuzz','BearSentiment','BullSentiment']]
    df_final=df_cols.sort_values(by=['BullSentiment'],ascending=False).reset_index()



fetchDataAPI = PythonOperator(
        task_id="fetch_data_from_API",
        python_callable=fetchDataAPI,
        dag=dag
    )
datatransform = PythonOperator(
        task_id="datatransform",
        python_callable=datatransform,
        dag=dag
    )
    
send_email = EmailOperator(
        task_id='send_email',
        to='leandersavio@gmail.com',
        subject='Airflow Alert',
        html_content=""" <h3>HELLO,</h3><br><h3> PIPELINE SUCESSFULL</h3> """,
        dag=dag
)

  
    
fetchDataAPI >> datatransform >> send_email

```


